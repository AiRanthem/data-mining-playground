{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part2. 对segment数据集进行分类"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取和预处理\n",
    "segment数据集是arff格式文件，需要使用scipy来读取。\n",
    "由于train数据集有1500条，test数据集也有800多条，这样分割有点浪费。这里把它们合并起来重新按照20%的比例划分训练集和测试集"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     region-centroid-col  region-centroid-row  region-pixel-count  \\\n0                   38.0                189.0                 9.0   \n1                   25.0                199.0                 9.0   \n2                   49.0                139.0                 9.0   \n3                   63.0                220.0                 9.0   \n4                  161.0                135.0                 9.0   \n..                   ...                  ...                 ...   \n805                221.0                111.0                 9.0   \n806                 44.0                 79.0                 9.0   \n807                230.0                 41.0                 9.0   \n808                217.0                 77.0                 9.0   \n809                 53.0                134.0                 9.0   \n\n     short-line-density-5  short-line-density-2  vedge-mean   vegde-sd  \\\n0                     0.0                   0.0    1.000000   0.222222   \n1                     0.0                   0.0    1.111110   0.607407   \n2                     0.0                   0.0    0.166667   0.077778   \n3                     0.0                   0.0    3.055560  15.263000   \n4                     0.0                   0.0    0.055556   0.136083   \n..                    ...                   ...         ...        ...   \n805                   0.0                   0.0    0.611111   0.240741   \n806                   0.0                   0.0    0.444444   0.344265   \n807                   0.0                   0.0    0.888891   0.688530   \n808                   0.0                   0.0    1.555560   2.740740   \n809                   0.0                   0.0    2.166670   0.300000   \n\n     hedge-mean   hedge-sd  intensity-mean  rawred-mean  rawblue-mean  \\\n0      6.222220  33.318500       29.074100    26.333300      35.22220   \n1      1.055560   0.462963       17.518500    13.111100      17.88890   \n2      0.333333   0.088889        0.444444     0.000000       1.33333   \n3      3.666670   6.088890        8.185190     6.555560       6.44444   \n4      0.111111   0.172133        1.259260     0.777778       3.00000   \n..          ...        ...             ...          ...           ...   \n805    0.388889   0.240741        1.333330     0.000000       4.00000   \n806    0.777779   0.403686      107.741000    93.888900     126.55600   \n807    1.888890   1.241270      121.481000   110.222000     138.88900   \n808    1.666670   0.533333       40.222200    37.222200      48.22220   \n809    0.277778   0.018519        6.962960     4.666670      11.22220   \n\n     rawgreen-mean  exred-mean  exblue-mean  exgreen-mean  value-mean  \\\n0          25.6667    -8.22222     18.44440     -10.22220    35.22220   \n1          21.5556   -13.22220      1.11111      12.11110    21.55560   \n2           0.0000    -1.33333      2.66667      -1.33333     1.33333   \n3          11.5556    -4.88889     -5.22222      10.11110    11.55560   \n4           0.0000    -1.44444      5.22222      -3.77778     3.00000   \n..             ...         ...          ...           ...         ...   \n805         0.0000    -4.00000      8.00000      -4.00000     4.00000   \n806       102.7780   -41.55560     56.44440     -14.88890   126.55600   \n807       115.3330   -33.77780     52.22220     -18.44440   138.88900   \n808        35.2222    -9.00000     24.00000     -15.00000    48.22220   \n809         5.0000    -6.88889     12.77780      -5.88889    11.22220   \n\n     saturation-mean  hue-mean       class  \n0           0.271208  -2.04915     b'path'  \n1           0.393002   2.69011    b'grass'  \n2           0.777778  -2.09440  b'foliage'  \n3           0.486717   2.09315    b'grass'  \n4           1.000000  -1.82221   b'window'  \n..               ...       ...         ...  \n805         1.000000  -2.09440  b'foliage'  \n806         0.258079  -2.37797      b'sky'  \n807         0.206392  -2.28080      b'sky'  \n808         0.269192  -1.93207   b'cement'  \n809         0.587234  -2.13846   b'window'  \n\n[2310 rows x 20 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>region-centroid-col</th>\n      <th>region-centroid-row</th>\n      <th>region-pixel-count</th>\n      <th>short-line-density-5</th>\n      <th>short-line-density-2</th>\n      <th>vedge-mean</th>\n      <th>vegde-sd</th>\n      <th>hedge-mean</th>\n      <th>hedge-sd</th>\n      <th>intensity-mean</th>\n      <th>rawred-mean</th>\n      <th>rawblue-mean</th>\n      <th>rawgreen-mean</th>\n      <th>exred-mean</th>\n      <th>exblue-mean</th>\n      <th>exgreen-mean</th>\n      <th>value-mean</th>\n      <th>saturation-mean</th>\n      <th>hue-mean</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>38.0</td>\n      <td>189.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>0.222222</td>\n      <td>6.222220</td>\n      <td>33.318500</td>\n      <td>29.074100</td>\n      <td>26.333300</td>\n      <td>35.22220</td>\n      <td>25.6667</td>\n      <td>-8.22222</td>\n      <td>18.44440</td>\n      <td>-10.22220</td>\n      <td>35.22220</td>\n      <td>0.271208</td>\n      <td>-2.04915</td>\n      <td>b'path'</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>25.0</td>\n      <td>199.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.111110</td>\n      <td>0.607407</td>\n      <td>1.055560</td>\n      <td>0.462963</td>\n      <td>17.518500</td>\n      <td>13.111100</td>\n      <td>17.88890</td>\n      <td>21.5556</td>\n      <td>-13.22220</td>\n      <td>1.11111</td>\n      <td>12.11110</td>\n      <td>21.55560</td>\n      <td>0.393002</td>\n      <td>2.69011</td>\n      <td>b'grass'</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>49.0</td>\n      <td>139.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.166667</td>\n      <td>0.077778</td>\n      <td>0.333333</td>\n      <td>0.088889</td>\n      <td>0.444444</td>\n      <td>0.000000</td>\n      <td>1.33333</td>\n      <td>0.0000</td>\n      <td>-1.33333</td>\n      <td>2.66667</td>\n      <td>-1.33333</td>\n      <td>1.33333</td>\n      <td>0.777778</td>\n      <td>-2.09440</td>\n      <td>b'foliage'</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>63.0</td>\n      <td>220.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.055560</td>\n      <td>15.263000</td>\n      <td>3.666670</td>\n      <td>6.088890</td>\n      <td>8.185190</td>\n      <td>6.555560</td>\n      <td>6.44444</td>\n      <td>11.5556</td>\n      <td>-4.88889</td>\n      <td>-5.22222</td>\n      <td>10.11110</td>\n      <td>11.55560</td>\n      <td>0.486717</td>\n      <td>2.09315</td>\n      <td>b'grass'</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>161.0</td>\n      <td>135.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.055556</td>\n      <td>0.136083</td>\n      <td>0.111111</td>\n      <td>0.172133</td>\n      <td>1.259260</td>\n      <td>0.777778</td>\n      <td>3.00000</td>\n      <td>0.0000</td>\n      <td>-1.44444</td>\n      <td>5.22222</td>\n      <td>-3.77778</td>\n      <td>3.00000</td>\n      <td>1.000000</td>\n      <td>-1.82221</td>\n      <td>b'window'</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>805</th>\n      <td>221.0</td>\n      <td>111.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.611111</td>\n      <td>0.240741</td>\n      <td>0.388889</td>\n      <td>0.240741</td>\n      <td>1.333330</td>\n      <td>0.000000</td>\n      <td>4.00000</td>\n      <td>0.0000</td>\n      <td>-4.00000</td>\n      <td>8.00000</td>\n      <td>-4.00000</td>\n      <td>4.00000</td>\n      <td>1.000000</td>\n      <td>-2.09440</td>\n      <td>b'foliage'</td>\n    </tr>\n    <tr>\n      <th>806</th>\n      <td>44.0</td>\n      <td>79.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.444444</td>\n      <td>0.344265</td>\n      <td>0.777779</td>\n      <td>0.403686</td>\n      <td>107.741000</td>\n      <td>93.888900</td>\n      <td>126.55600</td>\n      <td>102.7780</td>\n      <td>-41.55560</td>\n      <td>56.44440</td>\n      <td>-14.88890</td>\n      <td>126.55600</td>\n      <td>0.258079</td>\n      <td>-2.37797</td>\n      <td>b'sky'</td>\n    </tr>\n    <tr>\n      <th>807</th>\n      <td>230.0</td>\n      <td>41.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.888891</td>\n      <td>0.688530</td>\n      <td>1.888890</td>\n      <td>1.241270</td>\n      <td>121.481000</td>\n      <td>110.222000</td>\n      <td>138.88900</td>\n      <td>115.3330</td>\n      <td>-33.77780</td>\n      <td>52.22220</td>\n      <td>-18.44440</td>\n      <td>138.88900</td>\n      <td>0.206392</td>\n      <td>-2.28080</td>\n      <td>b'sky'</td>\n    </tr>\n    <tr>\n      <th>808</th>\n      <td>217.0</td>\n      <td>77.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.555560</td>\n      <td>2.740740</td>\n      <td>1.666670</td>\n      <td>0.533333</td>\n      <td>40.222200</td>\n      <td>37.222200</td>\n      <td>48.22220</td>\n      <td>35.2222</td>\n      <td>-9.00000</td>\n      <td>24.00000</td>\n      <td>-15.00000</td>\n      <td>48.22220</td>\n      <td>0.269192</td>\n      <td>-1.93207</td>\n      <td>b'cement'</td>\n    </tr>\n    <tr>\n      <th>809</th>\n      <td>53.0</td>\n      <td>134.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.166670</td>\n      <td>0.300000</td>\n      <td>0.277778</td>\n      <td>0.018519</td>\n      <td>6.962960</td>\n      <td>4.666670</td>\n      <td>11.22220</td>\n      <td>5.0000</td>\n      <td>-6.88889</td>\n      <td>12.77780</td>\n      <td>-5.88889</td>\n      <td>11.22220</td>\n      <td>0.587234</td>\n      <td>-2.13846</td>\n      <td>b'window'</td>\n    </tr>\n  </tbody>\n</table>\n<p>2310 rows × 20 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "data1 = arff.loadarff('data/segment-tain.txt')\n",
    "data2 = arff.loadarff('data/segment-test.txt')\n",
    "df = pd.concat([pd.DataFrame(data1[0]),pd.DataFrame(data2[0])],axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize function\n",
    "def z_score_normalize(series):\n",
    "    '''\n",
    "    Params:  a pandas serious\n",
    "\n",
    "    Return: normalized version of the series\n",
    "    '''\n",
    "    return (series-series.mean())/series.std()\n",
    "\n",
    "# to train a classifier\n",
    "def train(clf, xtrain, ytrain, xtest, ytest):\n",
    "    '''\n",
    "    Params: a classifier object and dataset splited\n",
    "\n",
    "    Return: a classifier trained\n",
    "    '''\n",
    "    %time clf.fit(xtrain, ytrain)\n",
    "    train_accuracy = clf.score(xtrain, ytrain)\n",
    "    test_accuracy = clf.score(xtest, ytest)\n",
    "    print(\"accuracy on training set =\", train_accuracy)\n",
    "    print(\"accuracy on testing  set =\", test_accuracy)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为要使用多项式贝叶斯模型，所以预处理归一化到`[0,1]`"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "shape of xtrain (1848, 19)\nshape of xtest  (462, 19)\nshape of ytrain (1848,)\nshape of ytest  (462,)\n"
    }
   ],
   "source": [
    "data = df.iloc[:,:-1]\n",
    "target = df.iloc[:,-1]\n",
    "# preprocess the data\n",
    "data = MinMaxScaler().fit_transform(data)\n",
    "\n",
    "# preprocess the target\n",
    "class2int = { k:v for v,k in enumerate(list(target.drop_duplicates()))}\n",
    "int2class = { v:k for v,k in enumerate(list(target.drop_duplicates()))}\n",
    "target = target.replace(class2int)\n",
    "\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(data,target,test_size=0.2)\n",
    "\n",
    "print(\"shape of xtrain\" , xtrain.shape)\n",
    "print(\"shape of xtest \" , xtest.shape)\n",
    "print(\"shape of ytrain\" , ytrain.shape)\n",
    "print(\"shape of ytest \" , ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0      0\n1      1\n2      2\n3      1\n4      3\n      ..\n805    2\n806    4\n807    4\n808    6\n809    3\nName: class, Length: 2310, dtype: int64"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练各种分类器\n",
    "这是一个多类别问题，采取One VS Rest策略。\n",
    "\n",
    "从下面的现象看出，由于这个数据集相对复杂，分类器并没有像上一个数据集中一样集体达到完美的水平。所以我选择三种算法各训练一些模型，然后挑几个看起来比较好的模型去进行进一步评估，探究到底哪一种算法在这个数据集上表现更好。\n",
    "### 1. 决策树\n",
    "决策树的表现十分出色，虽然时间在所有算法中比较长，但达到了极高的正确率"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 72 ms\naccuracy on training set = 1.0\naccuracy on testing  set = 0.9567099567099567\n"
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(DecisionTreeClassifier())\n",
    "tree_clf = train(clf, xtrain, ytrain, xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 贝叶斯\n",
    "总体而言，贝叶斯算法在性能表现上及其优异，但是准确率比较差。尤其是连续性的数据，在假设属性为离散布尔型的伯努利模型上准确率出奇的低（但是反其道而行之也可以有另外两种模型差不多的表现）。我选择了表现最好的高斯模型进入下一步的评估。"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 16 ms\naccuracy on training set = 0.7851731601731602\naccuracy on testing  set = 0.8225108225108225\nWall time: 18 ms\naccuracy on training set = 0.770021645021645\naccuracy on testing  set = 0.7748917748917749\nWall time: 16 ms\naccuracy on training set = 0.21915584415584416\naccuracy on testing  set = 0.18614718614718614\n"
    }
   ],
   "source": [
    "# GaussianNB\n",
    "clf = OneVsRestClassifier(GaussianNB())\n",
    "bayes_clf = train(clf, xtrain, ytrain, xtest, ytest)\n",
    "\n",
    "# MultinomialNB\n",
    "clf = OneVsRestClassifier(MultinomialNB())\n",
    "_ = train(clf, xtrain, ytrain, xtest, ytest)\n",
    "\n",
    "# BernoulliNB\n",
    "clf = OneVsRestClassifier(BernoulliNB())\n",
    "_ = train(clf, xtrain, ytrain, xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. KNN\n",
    "和老师在课上讲的一样，很多时候是1聚类效果最好。"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "result of 1NN:\nWall time: 81 ms\naccuracy on training set = 1.0\naccuracy on testing  set = 0.9761904761904762\nresult of 3NN:\nWall time: 62 ms\naccuracy on training set = 0.9767316017316018\naccuracy on testing  set = 0.9675324675324676\nresult of 5NN:\nWall time: 55 ms\naccuracy on training set = 0.9691558441558441\naccuracy on testing  set = 0.9545454545454546\nresult of 7NN:\nWall time: 53 ms\naccuracy on training set = 0.9632034632034632\naccuracy on testing  set = 0.9567099567099567\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "OneVsRestClassifier(estimator=KNeighborsClassifier(algorithm='auto',\n                                                   leaf_size=30,\n                                                   metric='minkowski',\n                                                   metric_params=None,\n                                                   n_jobs=-1, n_neighbors=1,\n                                                   p=2, weights='uniform'),\n                    n_jobs=None)"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# KNN\n",
    "for i in [1,3,5,7]:\n",
    "    clf = OneVsRestClassifier(KNeighborsClassifier(n_neighbors=i, n_jobs=-1))\n",
    "    print(\"result of {}NN:\".format(i))\n",
    "    _ = train(clf, xtrain, ytrain, xtest, ytest)\n",
    "knn_1_clf = OneVsRestClassifier(KNeighborsClassifier(n_neighbors=1, n_jobs=-1))\n",
    "knn_1_clf.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三种候选分类器的进一步评估\n",
    "对分类器的评估方法有很多，我这里选择使用多分类常用的F1score来评估分类器。\n",
    "\n",
    "首先建立评估函数："
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate(clf, x, y, name):\n",
    "    '''\n",
    "    Params: classifier clf, data x, label y and the name of classifier.\n",
    "\n",
    "    Return: f1 and auc score of clf\n",
    "    '''\n",
    "    pred = clf.predict(x)\n",
    "    f1 = metrics.f1_score(pred, y, average='micro')\n",
    "    print(\"----ESTIMATING CLASSIFIER: {}----\".format(name))\n",
    "    print(\"----F1  Score = {} ----\".format(f1))\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后对每个分类器进行评估："
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----ESTIMATING CLASSIFIER: DecisionTree----\n----F1  Score = 0.9913419913419913 ----\n----ESTIMATING CLASSIFIER: Bayes----\n----F1  Score = 0.7926406926406926 ----\n----ESTIMATING CLASSIFIER: 1NN----\n----F1  Score = 0.9952380952380953 ----\n"
    }
   ],
   "source": [
    "_ = estimate(tree_clf, data, target, \"DecisionTree\")\n",
    "_ = estimate(bayes_clf, data, target, \"Bayes\")\n",
    "_ = estimate(knn_1_clf, data, target, \"1NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "综上所述：\n",
    "\n",
    "1. 这个数据集上，准确度决策树和1NN相当，3~7NN次之，贝叶斯最差，而计算性能反之。\n",
    "2. 三个候选分类其中，1NN表现略微好于决策树，贝叶斯较差。"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}