# 聚类算法

## 方法

### Partition Cluster 分区聚类

K-means, K-model

### Hierachical Clustering 层次聚类

AGNES，DIANA，BIRCH

## 应用

### k-means

#### 简介

 k-means算法中的k代表类簇个数，means代表类簇内数据对象的均值（这种均值是一种对类簇中心的描述），因此，k-means算法又称为k-均值算法。k-means算法是一种基于划分的聚类算法，以距离作为数据对象间相似性度量的标准，即数据对象间的距离越小，则它们的相似性越高，则它们越有可能在同一个类簇。数据对象间距离的计算有很多种，k-means算法通常采用欧氏距离来计算数据对象间的距离。

#### 详解

 k-means算法以距离作为数据对象间相似性度量的标准，通常采用**欧氏距离**来计算数据对象间的距离。下面给出欧式距离的计算公式：
$$
dist(x_{i},x_{j})=\sqrt{\sum_{d=1}^{D}(x_{i,d}-x_{j,d})^{2}}\ \ \ \ \ (1)
$$
其中，D表示数据对象的属性个数。
  k-means算法聚类过程中，每次迭代，对应的类簇中心需要重新计算（更新）：对应类簇中所有数据对象的均值，即为更新后该类簇的类簇中心。定义第k个类簇的类簇中心为Centerk，则类簇中心更新方式如下：
$$
Center_{k}=\frac{1}{|C_{k}|}\sum_{x_{i}\in C_{k}}x_{i}\ \ \ \ \ (2)
$$
其中，Ck表示第k个类簇，|Ck|表示第k个类簇中数据对象的个数，这里的求和是指类簇Ck中所有元素在每列属性上的和，因此Centerk也是一个含有D个属性的向量，表示为$Center_{k}=(Center_{k,1},Center_{k,2},...,Center_{k,D})$

  k-means算法需要不断地迭代来重新划分类簇，并更新类簇中心，那么迭代终止的条件是什么呢？一般情况，有两种方法来终止迭代：一种方法是设定迭代次数TT，当到达第TT次迭代，则终止迭代，此时所得类簇即为最终聚类结果；另一种方法是采用误差平方和准则函数，函数模型如下：
$$
J=\sum_{k=1}^{K}\sum_{x_{i}\in C_{k}}dist(x_{i},Center_{k})\ \ \ \ \ (3)
$$
其中，K表示类簇个数。当两次迭代J的差值小于某一阈值时，即ΔJ<δ时，则终止迭代，此时所得类簇即为最终聚类结果。

  k-means算法思想可描述为：首先初始化KK个类簇中心；然后计算各个数据对象到聚类中心的距离，把数据对象划分至距离其最近的聚类中心所在类簇中；接着根据所得类簇，更新类簇中心；然后继续计算各个数据对象到聚类中心的距离，把数据对象划分至距离其最近的聚类中心所在类簇中；接着根据所得类簇，继续更新类簇中心；……一直迭代，直到达到最大迭代次数TT，或者两次迭代JJ的差值小于某一阈值时，迭代终止，得到最终聚类结果。算法详细流程描述如下：
![](https://zty-pic-bed.oss-cn-shenzhen.aliyuncs.com/20200515141756.png)

#### 使用

`KMeans(n_clusters, init, n_init, max_iter, tol, precompute_distances, verbose, random_state, copy_x, n_jobs, algorithm)`

KMeans类的主要参数有：

　　　　1) **n_clusters**: 即我们的k值，一般需要多试一些值以获得较好的聚类效果。k值好坏的评估标准在下面会讲。

　　　　2）**max_iter**： 最大的迭代次数，一般如果是**凸数据集**的话可以不管这个值，如果数据集不是凸的，可能很难收敛，此时可以指定最大的迭代次数让算法可以及时退出循环。

　　　　3）**n_init：**用不同的初始化质心运行算法的次数。由于K-Means是结果受初始值影响的局部最优的迭代算法，因此需要多跑几次以选择一个较好的聚类效果，默认是10，一般不需要改。如果你的k值较大，则可以适当增大这个值。

　　　　4）**init：** 即初始值选择的方式，可以为完全随机选择'random',优化过的'k-means++'或者自己指定初始化的k个质心。一般建议使用默认的'k-means++'。

　　　　5）**algorithm**：有“auto”, “full” or “elkan”三种选择。"full"就是我们传统的K-Means算法， “elkan”是我们原理篇讲的elkan K-Means算法。默认的"auto"则会根据数据值是否是稀疏的，来决定如何选择"full"和“elkan”。一般数据是稠密的，那么就是 “elkan”，否则就是"full"。一般来说建议直接用默认的"auto"



适用于大数据集的MiniBatchKMeans 算法：

MiniBatchKMeans类的主要参数比KMeans类稍多，主要有：

　　　　1) **n_clusters**: 即我们的k值，和KMeans类的n_clusters意义一样。

　　　　2）**max_iter：**最大的迭代次数， 和KMeans类的max_iter意义一样。

　　　　3）**n_init：**用不同的初始化质心运行算法的次数。这里和KMeans类意义稍有不同，KMeans类里的n_init是用同样的训练集数据来跑不同的初始化质心从而运行算法。而MiniBatchKMeans类的n_init则是每次用不一样的采样数据集来跑不同的初始化质心运行算法。

　　　　4）**batch_size**：即用来跑Mini Batch KMeans算法的采样集的大小，默认是100.如果发现数据集的类别较多或者噪音点较多，需要增加这个值以达到较好的聚类效果。

　　　　5）**init：** 即初始值选择的方式，和KMeans类的init意义一样。

　　　　6）**init_size:** 用来做质心初始值候选的样本个数，默认是batch_size的3倍，一般用默认值就可以了。

　　　　7）**reassignment_ratio:** 某个类别质心被重新赋值的最大次数比例，这个和max_iter一样是为了控制算法运行时间的。这个比例是占样本总数的比例，乘以样本总数就得到了每个类别质心可以重新赋值的次数。如果取值较高的话算法收敛时间可能会增加，尤其是那些暂时拥有样本数较少的质心。默认是0.01。如果数据量不是超大的话，比如1w以下，建议使用默认值。如果数据量超过1w，类别又比较多，可能需要适当减少这个比例值。具体要根据训练集来决定。

　　　　8）**max_no_improvement：**即连续多少个Mini Batch没有改善聚类效果的话，就停止算法， 和reassignment_ratio， max_iter一样是为了控制算法运行时间的。默认是10.一般用默认值就足够了。

#### 调优方法

1. 归一化，标准化
2. 调k值

### DBSCAN

DBSCAN 算法是一种基于密度的聚类算法。

DBSCAN 算法是一种基于密度的聚类算法：
　　1.聚类的时候不需要预先指定簇的个数
　　2.最终的簇的个数不确定
DBSCAN算法将数据点分为三类：
　　1.核心点：在半径Eps内含有超过MinPts数目的点。
　　2.边界点：在半径Eps内点的数量小于MinPts,但是落在核心点的邻域内的点。
　　3.噪音点：既不是核心点也不是边界点的点。

和传统的K-Means算法相比，DBSCAN最大的不同就是不需要输入类别数k，当然它最大的优势是可以发现任意形状的聚类簇，而不是像K-Means，一般仅仅使用于凸的样本集聚类。同时它在聚类的同时还可以找出异常点，这点和BIRCH算法类似。

一般来说，如果数据集是稠密的，并且数据集不是凸的，那么用DBSCAN会比K-Means聚类效果好很多。如果数据集不是稠密的，则不推荐用DBSCAN来聚类。

　　DBSCAN的主要优点有：

　　　　1） 可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。

　　　　2） 可以在聚类的同时发现异常点，对数据集中的异常点不敏感。

　　　　3） 聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。

　　DBSCAN的主要缺点有：

　　　　1）如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。

　　　　2） 如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。

　　　　3） 调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值ϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。

#### 调参

​      		  1）**eps**： DBSCAN算法参数，即ϵ-邻域的距离阈值，和样本距离超过ϵϵ的样本点不在ϵϵ-邻域内。默认值是0.5.一般需要通过在多组值里面选择一个合适的阈值。eps过大，则更多的点会落在核心对象的ϵϵ-邻域，此时我们的类别数可能会减少， 本来不应该是一类的样本也会被划为一类。反之则类别数可能会增大，本来是一类的样本却被划分开。

　　　　2）**min_samples**： DBSCAN算法参数，即样本点要成为核心对象所需要的ϵϵ-邻域的样本数阈值。默认值是5. 一般需要通过在多组值里面选择一个合适的阈值。通常和eps一起调参。在eps一定的情况下，min_samples过大，则核心对象会过少，此时簇内部分本来是一类的样本可能会被标为噪音点，类别数也会变多。反之min_samples过小的话，则会产生大量的核心对象，可能会导致类别数过少。

　　　　3）**metric**：最近邻距离度量参数。可以使用的距离度量较多，一般来说DBSCAN使用默认的欧式距离（即p=2的闵可夫斯基距离）就可以满足我们的需求。可以使用的距离度量参数有：

　　　　a) 欧式距离 “euclidean”: ![\sqrt{\sum\limits_{i=1}^{n}(x_i-y_i)^2}](https://private.codecogs.com/gif.latex?%5Csqrt%7B%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bn%7D%28x_i-y_i%29%5E2%7D)

　　　　b) 曼哈顿距离 “manhattan”： ![\sum\limits_{i=1}^{n}|x_i-y_i|](https://private.codecogs.com/gif.latex?%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bn%7D%7Cx_i-y_i%7C)

　　　　c) 切比雪夫距离“chebyshev”: ![max|x_i-y_i| (i = 1,2,...n)](https://private.codecogs.com/gif.latex?max%7Cx_i-y_i%7C%20%28i%20%3D%201%2C2%2C...n%29)

　　　　d) 闵可夫斯基距离 “minkowski”: ![\sqrt[p]{\sum\limits_{i=1}^{n}(|x_i-y_i|)^p}](https://private.codecogs.com/gif.latex?%5Csqrt%5Bp%5D%7B%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bn%7D%28%7Cx_i-y_i%7C%29%5Ep%7D) p=1为曼哈顿距离， p=2为欧式距离。

　　　　e) 带权重闵可夫斯基距离 “wminkowski”: ![\sqrt[p]{\sum\limits_{i=1}^{n}(w*|x_i-y_i|)^p}](https://private.codecogs.com/gif.latex?%5Csqrt%5Bp%5D%7B%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bn%7D%28w*%7Cx_i-y_i%7C%29%5Ep%7D)其中w为特征权重

　　　　f) 标准化欧式距离 “seuclidean”: 即对于各特征维度做了归一化以后的欧式距离。此时各样本特征维度的均值为0，方差为1.

　　　　g) 马氏距离“mahalanobis”：![\sqrt{(x-y)^TS^{-1}(x-y)}](https://private.codecogs.com/gif.latex?%5Csqrt%7B%28x-y%29%5ETS%5E%7B-1%7D%28x-y%29%7D) 其中，![S^{-1}](https://private.codecogs.com/gif.latex?S%5E%7B-1%7D) 为样本协方差矩阵的逆矩阵。当样本分布独立时， S为单位矩阵，此时马氏距离等同于欧式距离。

　　还有一些其他不是实数的距离度量，一般在DBSCAN算法用不上，这里也就不列了。

　　　　4）**algorithm**：最近邻搜索算法参数，算法一共有三种，第一种是蛮力实现，第二种是KD树实现，第三种是球树实现。这三种方法在[K近邻法(KNN)原理小结](http://www.cnblogs.com/pinard/p/6061661.html)中都有讲述，如果不熟悉可以去复习下。对于这个参数，一共有4种可选输入，‘brute’对应第一种蛮力实现，‘kd_tree’对应第二种KD树实现，‘ball_tree’对应第三种的球树实现， ‘auto’则会在上面三种算法中做权衡，选择一个拟合最好的最优算法。需要注意的是，如果输入样本特征是稀疏的时候，无论我们选择哪种算法，最后scikit-learn都会去用蛮力实现‘brute’。个人的经验，一般情况使用默认的 ‘auto’就够了。 如果数据量很大或者特征也很多，用"auto"建树时间可能会很长，效率不高，建议选择KD树实现‘kd_tree’，此时如果发现‘kd_tree’速度比较慢或者已经知道样本分布不是很均匀时，可以尝试用‘ball_tree’。而如果输入样本是稀疏的，无论你选择哪个算法最后实际运行的都是‘brute’。

　　　　5）**leaf_size**：最近邻搜索算法参数，为使用KD树或者球树时， 停止建子树的叶子节点数量的阈值。这个值越小，则生成的KD树或者球树就越大，层数越深，建树时间越长，反之，则生成的KD树或者球树会小，层数较浅，建树时间较短。默认是30. 因为这个值一般只影响算法的运行速度和使用内存大小，因此一般情况下可以不管它。

　　　　6） **p**: 最近邻距离度量参数。只用于闵可夫斯基距离和带权重闵可夫斯基距离中p值的选择，p=1为曼哈顿距离， p=2为欧式距离。如果使用默认的欧式距离不需要管这个参数。

　　　　以上就是DBSCAN类的主要参数介绍，其实**需要调参的就是两个参数eps和min_samples**，这两个值的组合对最终的聚类效果有很大的影响。

### BIRCH

BIRCH算法的主要优点有：

　　　　1) 节约内存，所有的样本都在磁盘上，CF Tree仅仅存了CF节点和对应的指针。

　　　　2) 聚类速度快，只需要一遍扫描训练集就可以建立CF Tree，CF Tree的增删改都很快。

　　　　3) 可以识别噪音点，还可以对数据集进行初步分类的预处理

BIRCH算法的主要缺点有：

　　　　1) 由于CF Tree对每个节点的CF个数有限制，导致聚类的结果可能和真实的类别分布不同.

　　　　2) 对高维特征的数据聚类效果不好。此时可以选择Mini Batch K-Means

　　　　3) 如果数据集的分布簇不是类似于超球体，或者说不是凸的，则聚类效果不好。



**BIRCH参数：**

​     1) **threshold**:即叶节点每个CF的最大样本半径阈值T，它决定了每个CF里所有样本形成的超球体的半径阈值。一般来说threshold越小，则CF Tree的建立阶段的规模会越大，即BIRCH算法第一阶段所花的时间和内存会越多。但是选择多大以达到聚类效果则需要通过调参决定。默认值是0.5.如果样本的方差较大，则一般需要增大这个默认值。

​	2) **branching_factor**：即CF Tree内部节点的最大CF数B，以及叶子节点的最大CF数L。这里scikit-learn对这两个参数进行了统一取值。也就是说，branching_factor决定了CF Tree里所有节点的最大CF数。默认是50。如果样本量非常大，比如大于10万，则一般需要增大这个默认值。选择多大的branching_factor以达到聚类效果则需要通过和threshold一起调参决定

　3）**n_clusters**：即类别数K，在BIRCH算法是可选的，如果类别数非常多，我们也没有先验知识，则一般输入None，此时BIRCH算法第4阶段不会运行。但是如果我们有类别的先验知识，则推荐输入这个可选的类别值。默认是3，即最终聚为3类。

　4）**compute_labels**：布尔值，表示是否标示类别输出，默认是True。一般使用默认值挺好，这样可以看到聚类效果。

### AP聚类算法

​    damping : 衰减系数，默认为 0.5
​    convergence_iter : 迭代次后聚类中心没有变化，算法结束，默认为15.
​    max_iter : 最大迭代次数，默认200.
​    copy : 是否在元数据上进行计算，默认True，在复制后的数据上进行计算。
​    preference : S的对角线上的值
​    affinity :S矩阵（相似度），默认为euclidean（欧氏距离）矩阵，即对传入的X计算距离矩阵，也可以设置为precomputed，那么X就作为相似度矩阵。

### 谱聚类算法

比起传统的K-Means算法，谱聚类对数据分布的适应性更强，聚类效果也很优秀，同时聚类的计算量也小很多，更加难能可贵的是实现起来也不复杂。

谱聚类算法的优缺点：

谱聚类算法的主要优点有：

　　　　1）谱聚类只需要数据之间的相似度矩阵，因此对于处理稀疏数据的聚类很有效。这点传统聚类算法比如K-Means            很难做到

　　　　2）由于使用了降维，因此在处理高维数据聚类时的复杂度比传统聚类算法好。

谱聚类算法的主要缺点有：

　　　　1）如果最终聚类的维度非常高，则由于降维的幅度不够，谱聚类的运行速度和最后的聚类效果均不好。

　　　　2) 聚类效果依赖于相似矩阵，不同的相似矩阵得到的最终聚类效果可能很不同。

# 降维

## 主成分分析PCA

　　　　现在我们对sklearn.decomposition.PCA的主要参数做一个介绍：

　　　　1）**n_components**：这个参数可以帮我们指定希望PCA降维后的特征维度数目。最常用的做法是直接指定降维到的维度数目，此时n_components是一个大于等于1的整数。当然，我们也可以指定主成分的方差和所占的最小比例阈值，让PCA类自己去根据样本特征方差来决定降维到的维度数，此时n_components是一个（0，1]之间的数。当然，我们还可以将参数设置为"mle", 此时PCA类会用MLE算法根据特征的方差分布情况自己去选择一定数量的主成分特征来降维。我们也可以用默认值，即不输入n_components，此时n_components=min(样本数，特征数)。

　　　　2）**whiten** ：判断是否进行白化。所谓白化，就是对降维后的数据的每个特征进行归一化，让方差都为1.对于PCA降维本身来说，一般不需要白化。如果你PCA降维后有后续的数据处理动作，可以考虑白化。默认值是False，即不进行白化。

　　　　3）**svd_solver**：即指定奇异值分解SVD的方法，由于特征分解是奇异值分解SVD的一个特例，一般的PCA库都是基于SVD实现的。有4个可以选择的值：{‘auto’, ‘full’, ‘arpack’, ‘randomized’}。randomized一般适用于数据量大，数据维度多同时主成分数目比例又较低的PCA降维，它使用了一些加快SVD的随机算法。 full则是传统意义上的SVD，使用了scipy库对应的实现。arpack和randomized的适用场景类似，区别是randomized使用的是scikit-learn自己的SVD实现，而arpack直接使用了scipy库的sparse SVD实现。默认是auto，即PCA类会自己去在前面讲到的三种算法里面去权衡，选择一个合适的SVD算法来降维。一般来说，使用默认值就够了。

　　　　除了这些输入参数外，有两个PCA类的成员值得关注。第一个是**explained_variance_**，它代表降维后的各主成分的方差值。方差值越大，则说明越是重要的主成分。第二个是**explained_variance_ratio_**，它代表降维后的各主成分的方差值占总方差值的比例，这个比例越大，则越是重要的主成分。